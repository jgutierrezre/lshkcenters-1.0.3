from .BaseHashing import BaseHashing

from collections import defaultdict
import networkx as nx
import numpy as np


class LSH(BaseHashing):
    def __init__(self, *args, **kwargs) -> None:
        """
        Initializes the LSH object.
        """
        super().__init__(*args, **kwargs)

    # ==========================
    # Hash Table
    # ==========================

    def _init_hash(self) -> None:
        """
        Initialize the hash table.
        """
        # Get the similarity matrices for each attribute.
        similarity_matrices = self.measure.get_similarity_matrices()
        print("similarity_matrices\n", similarity_matrices)

        # Generate the similarity graph and compute the partitions and cut values generated by the
        # Stoer-Wagner algorithm.
        partitions, cut_values_normal = self._compute_graph_partitions(
            similarity_matrices
        )
        print("partitions\n", partitions)
        print("cut_values_normal\n", cut_values_normal)

        # Get the hbits attribute indexes with the highest cut values.
        bit_indices = np.argpartition(cut_values_normal, -self.hbits)[-self.hbits :]
        print("bit_indices\n", bit_indices)

        # Compute the hash table.
        hash_table = self._compute_hash_table(partitions, bit_indices)
        print("hash_table\n", hash_table)

        self._hash_table = hash_table

    def _compute_graph_partitions(
        self, similarity_matrices: list
    ) -> tuple[list, np.ndarray]:
        """
        Generate similarity graph using provided similarity matrices.
        Uses Stoer-Wagner algorithm to find the maximum cuts on the graphs.

        Args:
            similarity_matrices (list): List of similarity matrices for each attribute.

        Returns:
            tuple[list, np.ndarray]: List of partitions and cut values for each attribute.
        """

        partitions = []

        # cut_values = np.full((self.d), np.inf, dtype=float)
        cut_values_normal = np.full((self.d), np.inf, dtype=float)

        # Iterate through each matrix in simMatrix using enumerate
        for i, matrix in enumerate(similarity_matrices):
            # Create graph G
            G = nx.from_numpy_array(np.triu(matrix, 1))

            # If there's more than 1 node, compute the cut
            if len(G.nodes) > 1:
                ut_value, partition = nx.stoer_wagner(G)
                partitions.append(partition)
                # cut_values[i] = ut_value
                cut_values_normal[i] = ut_value / self.D[i]
            else:
                partitions.append([[0], [0]])

        return partitions, cut_values_normal

    def _compute_hash_table(
        self, partitions: list, attribute_indices: np.ndarray
    ) -> dict:
        hash_table = defaultdict(list)
        """
        Generate hash table using provided partitions and bit indexes.
        
        Args:
            partitions (list): List of partitions for each attribute.
            attribute_indices (np.ndarray): List of indices of attributes with the highest cut values.
            
        Returns:
            hash_table: Hash table containing the hash values and the corresponding data points.
        """

        # For each data point in the dataset
        for data_point_idx, data_point in enumerate(self.X):
            hash_value = 0  # Initialize hash value for the current data point to 0

            for bit_position, attribute_idx in enumerate(attribute_indices):
                # Extract partition 0 (A_{d0}) and partition 1 (A_{d1}) for the dth attribute
                partition_0, partition_1 = partitions[attribute_idx]
                # If the value is in partition 1, set the corresponding bit to 1
                if data_point[attribute_idx] in partition_1:
                    hash_value |= 1 << bit_position

            hash_table[hash_value].append(data_point_idx)

        hash_table = dict(hash_table)

        return hash_table
